% Chapter 4

\chapter{ Nucleus }
\label{Chapter4}
\lhead{Chapter 4. \emph{ Nucleus }}

\section{Overall structure}

TODO: Nucleus is like an ogre. It has layers. The lowest layer hides the complexity of the underlying API and makes the higher layers API-agnostic via complete isolation.

TODO: On top of it lies the high level rendering interface where the client programmer manages objects, materials, surfaces, etc.

TODO: Aside from that, there's a post-processing framework and a spatial subdivision package for quick scene visibility queries.

\section{Low-level abstraction layer}

TODO: The OpenGL API is messy, Cg messy, must have a convenience wrapper. Implemented one that lies on a similar layer as XNA. Possible to use it directly without the rest of the system.

TODO:
	
This layer completely isolates the underlying rendering API \emph{(currently OpenGL 3.3 + CgFX)} and gives an easy to use interface on top of it. The main building blocks are:
	
\begin{description}

\item[Effect] --- wraps vertex, geometry and fragment shaders together, allows shader specialization (e.g. setting the implementations of Cg interfaces, providing sizes for unsized Cg arrays). Once compiled, allows the programmer to provide values for uniform parameters shared by all instances of the effect. Finally, it allows instantiation into EffectInstances.

\item[EffectInstance] --- provides a name space and storage for uniform and varying parameters associated with a single instance of an Effect. Additionally, uses a VertexArray to cache current varying parameter bindings. Automatically performs reference counting of resources assigned as uniform (Textures) and varying parameters (VertexBuffers). The memory for EffectInstances is allocated from pools in contiguous slabs for both the structure data and the effect-dependent payload. By adding an indirection to parameter storage, multiple Effects and EffectInstances may source their parameters from the same variable in memory, possibly managed entirely by the client.

\item[Buffer] ---allows the specification and modification of graphics API-allocated memory (VRAM, AGP, system).

\begin{description}
\item[VertexBuffer] --- used for generic untyped vertex attributes. The actual types are specified when binding varying parameters to EffectInstances and specifying the offset, stride and type of the underlying data.
\item[IndexBuffer] --- specialized for 16 and 32-bit mesh indices.
\item[UniformBuffer] --- bindable uniform / constant buffers for shader programs.
\end{description}

\item[VertexArray] --- a special kind of a driver-level object that caches varying parameter bindings, allowing for reduction of API calls.

\item[Framebuffer] --- wrapper over all classes of framebuffers with automatic usage of Multiple Render Targets, rendering to textures, floating point support, etc.

\item[Texture] --- 1D, 2D, 3D, rectangle, cube, \ldots

\end{description}

The aforementioned resources are accessed via opaque handles created and managed by a \textbf{Renderer}, eliminating the potential of fatal user mistakes such as manual disposal of a resource and its subsequent usage or memory corruption.

Additionally, the \textbf{Renderer} provides functionality to create, dispose and render the contents of a \textbf{RenderList} according to the current \textbf{RenderState}.

The \textbf{RenderList} is a collection of indices (ordinals) of EffectInstances and basic associated data required to render them, including:
	
\begin{itemize}
\item Model $\rightarrow$ world and world $\rightarrow$ model transformation 4Ã—3 matrices
\item An IndexBuffer along with the count of indices to use, offset, min and max therein
\item The number of instances of the object to be rendered via \emph{hardware instancing}
\item Mesh topology
\end{itemize}

An important factor to note is that the RenderList does not plainly contain EffectInstances, but rather their \emph{rendering ordinals}. The ordinals are u32 numbers assigned to each effect instance and managed internally by the renderer. Their order is determined by a heuristic attempting to minimize the number of state changes required when rendering the instances (currently: sorting by texture handles). This means that once the render list is constructed, the algorithm to minimize the required state changes basically boils down to \emph{sorting the ordinals}, which is a very cheap operation. More ordering options will be made available in the future, so that the objects may be sorted by distance or using per-Effect routines built with knowledge of their performance characteristics.

As mentioned before, the \textbf{Renderer} also gives access to the \textbf{RenderState}, a set of common rendering settings, such as the mode of Z-testing, blending, back-face culling, etc.

The Graphics base layer can therefore be used instead of the underlying graphics API, completely hiding its complexity and error-prone setup. It also makes it possible to change the backend without introducing client code. Implementation of a \textbf{Direct3D}-based \textbf{Renderer} is one possible area of future research.
	
\section{Kernels}

The basic building block of the high level rendering interface of Nucleus is called a \emph{kernel}. Conceptually, it is just a function. The implementation of a kernel may be specified using a code snippet (the current implementation allows the Cg language for this purpose). The more interesting option is to define it using a graph. Specifically, a directed acyclic graph whose nodes are either kernels or special-purpose entities. The special-purpose nodes can be used to denote what constitutes the input and output parameters of a graph-based kernel, as well as to request external data for it.

While first-class support of graphs does not result in more expressive power than of regular function-based composition, it provides a comfortable mental and programmatic framework for such composition. It enables the use of classical graph algorithms and makes development of authoring tools easier.

As a convention, the return values of kernels are called \emph{output parameters} and denoted by the \textbf{out} keyword in the kernel signature. Conversely, regular kernel parameters are called \emph{input parameters} and optionally denoted by the \textbf{in} keyword. This is modeled after the Cg shading language.

Within a kernel graph, connections may exist between particular nodes, as well as between particular parameters of the nodes.

\subsection{Standard kernel types}

Similarly to how GPU-based shading requires the specification of shader programs for each of the processing domains, Nucleus demands that a kernel be provided for each of its domains. Where it differs drastically, is what the domains are. Each object to be rendered must have 3 kernels specified for it:

\begin{description}

\item[Structure] --- Defines the macro-structure and mostly contains what a vertex or geometry shader might. It's responsible for providing the primitives to be rendered, including data such as positions, normals, partial derivatives of position with respect to texture coordinates, etc.

\item[Reflectance] --- Enables the object to interact with lights by implementing a BRDF. Examples include the Lambertian model, Phong, Cook-Torrance and Ashikhmin-Shirley.

\item[Material] --- This is the level on which most artists will work, specifying the albedo of the rendered object, its bumpiness, specular tint, emissive lighting, roughness, etc. Kernels of this type will be created for types of objects in a scene, as well as for particular instances thereof.

\end{description}

Specifying these kernels for a renderable object boils down to designating a \emph{concrete} kernel with an identical signature for each of the above \emph{abstract} kernels.

A scene will usually also contain lights, each of which must be associated with a kernel. In this case, an implementation of the abstract Light. At program runtime, Light kernels are connected to the per-object kernel types. This mechanism allows specification of custom attenuation, sampling and shadowing algorithms, which automatically are applicable to any Reflectance kernel.

% TODO: link to info about the signatures of these kernels in some other chapter

Such a break-down of kernels may superficially look similar to the shader types GPUs require. \emph{Structure} kernels resemble \emph{vertex} shaders, \emph{Material} kernels resemble \emph{pixel} shaders. The first major distinction is that the signatures of the above kernels are fixed in Nucleus, hence each Material may be combined with any Reflectance, any Structure and any Light. The second important difference is that these kernels are not tied to any particular part of the pipeline. Instead, a \emph{renderer} is free to use these kernels in any rendering algorithm, as detailed in section "Renderers". TODO: proper link.

Another core kernel type is used in \emph{post-processing} operations and described in section "Post-Processing". TODO: proper link.

This particular break-down of processing domains is comparable to the \emph{RenderMan} model:

\begin{center}
\begin{tabular}{ | c | c | c | }
\hline
RenderMan shader & Nucleus kernel \\
\hline
Surface & Material + Reflectance \\
Light & Light \\
Imager & PostProcess \\
Displacement & Structure \\
Volume & --- \\
\hline
\end{tabular}
\end{center}

% TODO: ref http://www.vga.hr/resources/tutorials/3d/rsl/html/chapter_01.htm

Requiring that material and reflectance be defined by separate kernels gives Nucleus the ability to split the rendering equation into independent components and apply algorithms which utilize this split for performance gains in complex scene configurations. ( TODO: more info / link / ref to the Renderers section or to the Examples section. ). While it restricts the freedom an artist might have when using \emph{RenderMan}, other real-time rendering systems have often been restricted to just \emph{a single} reflectance model ( TODO: refs to stalker, killzone2, crysis? ).

TODO: note about volume shaders being future work

\section{Semantic type system}

% TODO: ref the related work section? maybe put semantic types into an index?
Nucleus uses \emph{semantic types} in order to describe the signatures of computational kernels. Each type is a set of \emph{trait} values and each \emph{trait} is an enumerated type in the classical meaning used in programming languages. Possible traits and their values include (TODO):
\begin{description}
\item[basis] --- world, view, clip, tangent,
\item[colorSpace] --- RGB, sRGB, logLUV,
\item[length] --- unit, any,
\item[linearity] --- linear, logarithmic, exponential,
\item[use] --- position, normal, color, uv,
\item[type] --- float, float2, float3, float4, float4x4, ... (Cg type)
\end{description}

Trait values of the same name are considered distinct if they belong to different traits. For example, value \emph{A} of trait \emph{X} is distinct from value \emph{A} of trait \emph{Y}. In order to make the definition mathematically sound, a trait value may be considered a $(trait, value)$ tuple.

A type is then any combination of trait values, e.g. \emph{"type float3 + use position + basis world"} and \emph{"type float2 + use uv"}, where the "+" sign denotes a delimiter. An empty set of trait values is a valid type as well.

\subsection{Type coercion}

A kernel graph specified by the user or constructed by an algorithm must be type-checked within Nucleus before it is used in any rendering operation. I first discuss how the process works on individual connections, then extend the result to complete kernel graphs.

Given a connection from an output parameter of type $X$ to a input parameter of type $Y$, the connection is determined to be valid if $Y \subseteq X$. Otherwise such a connection is a type mismatch. For example, connecting an output parameter of type \emph{"type float3 + use position"} to an input parameter of type \emph{"type float3"} is valid, however connecting \emph{"type float4"} to \emph{"type float3"} or to \emph{"type float4 + use color"} is a mismatch.

In the case of a mismatch, Nucleus will try to perform \emph{type coercion} by inserting additional computational kernels between the source and the destination. The kernels which it considers come from a specially designated set of \emph{semantic converters}. Each semantic converter is a kernel of a single input and a single output parameter. Additionally an integral \emph{cost} value is associated with each converter, which is an estimate of the computational complexity of the code within its definition.

It's possible to define a directed weighted graph $G=(V, E)$ such that its vertices, $V$ are all possible semantic types. The graph contains an edge $e$ from $a$ to $b$ if and only if there exists a converter, which turns type $a$ into a type which may be connected to type $b$ without a mismatch. The weight associated with $e$ is the cost value of the corresponding converter.

Because $G$ contains all possible semantic types, it must contain $X$ and $Y$. A search for the shortest path is performed using Dijkstra's algorithm. If a path from $X$ to $Y$ in $G$ does not exist, an error is reported. If two or more shortest paths are found, an ambiguity error is reported. Otherwise, the path defines a valid \emph{type coercion}. New nodes are created within the kernel graph and inserted in place of the original parameter connection.

% TODO: some pictures
	
\subsection{Automatic connection determination}

The kernel graph may contain connections not only between particular parameters, but also between kernels. The type-checking process converts these coarse-grained dependencies into individual parameter connections using an extension of the type coercion algorithm.

To automatically determine the parameter which should be connected to an input of a kernel graph node, all output parameters of all nodes connected to this node are considered. The possible type coercions are evaluated in parallel and the shortest one is selected. Similarly to the aforementioned algorithm, an ambiguous choice is an error.

% TODO: some pictures

\subsection{Semantic expressions}

TODO:

Nucleus also extends the technique proposed in Abstract Shade Trees by allowing the use of simple expressions which operate on semantic traits in the specification of output parameter semantics.

When working with a prototype of Nucleus which utilized a simpler version of the type system, it became apparent it's useful not only to compute values using kernels, but also let the output semantics of kernel functions depend on the input semantics of these functions and the kernels they're connected to. For instance, One could have a kernel which samples a texture. Textures may be tagged with traits, specifying what sort of data they contain. If such a texture is connected to the sampling kernel, it's crucial to be able to express that the type of the sample should retain the traits of the texture. This is enabled by semantic expressions.

The sampling kernel could have the following signature:

\begin{lstlisting}[frame=single]
Tex2D = kernel (
    in texture <type sampler2D>,
    in uv <type float2 + use uv>,
    out sample <in.texture.actual + type float4>
);
\end{lstlisting}

% TODO: change other quotes to the `` ... '' style

In this case, the traits of the output parameter ``sample'' depend on the actual parameter which is connected to the input ``texture'' parameter in a kernel graph. Hence, the result of connecting an input with a semantic <type sampler2D + use color> will be a sample with the semantic <type float4 + use color>.

TODO: subtractive expressions

\section{Renderers}

TODO: Renderers use Renderables and kernels associated with them to render meshes using arbitrary algorithms.

\section{Post-processing}

%Nucleus covers one more aspect of Renderman - the \emph{Imager} shaders in the form of kernel graphs which are automatically decomposed into post-processing operations.

TODO:

In addition to regular scene rendering, Nucleus contains special support for post-processing. It is performed by specifying a kernel and feeding it data. In this case, the data is just a texture (multiple inputs into the post-processing pipeline are planned for a later stage).

A simple post-processing pipeline might look like this:

TODO: img

Nodes using the special Blit kernel are used to break the graph to be rendered in multiple passes. This enables the implementation of algorithms such as the separable Gaussian blur. Blit nodes may also rescale the input as well as change its internal format.

In this case, the graph would be broken down into two passes:
	
TODO: img

TODO: In more complex cases, Nucleus will also find which passes may be performed at the same time and automatically use Multiple Render Targets.

\section{Functional composition}

TODO: 

% TODO: Linear data flow in the kernel DAG too restricting. Nucleus supports functional composition with currying instead.

Implementing post-processing using the framework of a regular graph-based editor is a tricky business. Consider the following graph:

TODO: img

The \textbf{Blur} kernel expects to get an image which it may sample multiple times, with various offsets. Connecting the \emph{Input} directly to \emph{Blur} would have done the trick, however the user has decided to filter the input through a power function first. The function operates not on Images, but on individual samples. Normally, the system would give up completely, however Nucleus has one more trick up its sleeve.

In this case, the \textbf{Image} type is nothing but a kernel. Particularly, one with the following signature:
	
\begin{lstlisting}[frame=single]
Image = kernel(
    in uv <type float2 + use uv>,
    out sample <type float4>
);
\end{lstlisting}

Hence, the Blur kernel above expects \emph{another kernel} as its input, but the parameter connected to this input is not a kernel. Its type, however, is the type of the kernel's output parameter. This special case causes the type system to consider the whole incoming graph for \emph{functional composition}. As a result, the graph might be turned into:

TODO: img

Naturally this very mechanism is not restricted to just the post-processing component of Nucleus and may be used e.g. in order to implement procedural texturing.
	
\section{Code generation}

TODO: Generates Cg code which is then introspected by the mid-level layer.
