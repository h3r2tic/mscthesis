% Chapter 4

\chapter{ Nucleus }
\label{Chapter4}
\lhead{Chapter 4. \emph{ Nucleus }}

\section{Overall structure}

At the highest level, the architecture of Nucleus is split in two layers. The first layer completely wraps an underlying rendering API, providing primitives which are easier to use and less error-prone, while retaining high efficiency.

The second layer is the high-level rendering interface which is the main topic of this thesis, described in further detail in this chapter.

Aside the above, Nucleus offers a post-processing framework, which in turn may use both of these layers.

\section{Low-level abstraction layer}
\label{sec:lowLevelInfo}

The high-level machinery of Nucleus may be built directly on top of a rendering API such as OpenGL or Direct3D, however a wrapper over these is desirable for numerous reasons:

\begin{enumerate}
\item The OpenGL API is portable and supported by multiple operating systems, however its implementation in drivers is currently less stable than that of Direct3D. Hence it is vital to be able to support both APIs.
\item Hardware may support rendering APIs to varying degrees via feature sets and extensions, yet workarounds can be created and exposed as a common set of features. Additionally, functionality required by the standards is implemented to varying degrees, and sometimes contains bugs. The wrapper allows these exceptional details to be caught in a single location.
\item Directly working with the graphics API is error-prone and may be difficult to debug. The wrapper can enforce particular usage patterns, allow straightforward resource management and reduce code duplication, all of which lead to faster and cheaper development.
\end{enumerate}

The lowest abstraction layer has been built on a level similar to Microsoft's XNA library and currently has a backend utilizing \emph{OpenGL 3.3} and NVIDIA's \emph{Cg Toolkit}. The main building blocks of the framework are:

\begin{description}

\item[Effect] --- wraps vertex, geometry and fragment shaders together, allows shader specialization (e.g. setting the implementations of Cg interfaces, providing sizes for unsized Cg arrays). Once compiled, allows the programmer to provide values for uniform parameters shared by all instances of the effect. Finally, it allows instantiation into EffectInstances.

\item[EffectInstance] --- provides a name space and storage for uniform and varying parameters associated with a single instance of an Effect. Additionally, uses a VertexArray to cache current varying parameter bindings. Automatically performs reference counting of resources assigned as uniform (Textures) and varying parameters (VertexBuffers). The memory for EffectInstances is allocated from pools in contiguous slabs for both the structure data and the effect-dependent payload. By adding an indirection to parameter storage, multiple Effects and EffectInstances may source their parameters from the same variable in memory, possibly managed entirely by the client. Typically, each object in the scene will own a single EffectInstance for each Effect it may be rendered with.

\item[Buffer] ---allows the specification and modification of graphics API-allocated memory (VRAM, AGP, system).

\begin{description}
\item[VertexBuffer] --- used for generic untyped vertex attributes. The actual types are specified when binding varying parameters to EffectInstances and specifying the offset, stride and type of the underlying data.
\item[IndexBuffer] --- specialized for 16 and 32-bit mesh indices.
\item[UniformBuffer] --- bindable uniform / constant buffers for shader programs.
\end{description}

\item[VertexArray] --- a special kind of a driver-level object that caches varying parameter bindings, allowing for reduction of API calls.

\item[Framebuffer] --- wrapper over all classes of framebuffers with automatic usage of Multiple Render Targets, rendering to textures, floating point support, etc.

\item[Texture] --- 1D, 2D, 3D, rectangle, cube, \ldots
\end{description}

The aforementioned resources are accessed via opaque handles created and managed by a \textbf{Renderer}, eliminating the potential of fatal user mistakes such as manual disposal of a resource and its subsequent usage or memory corruption. For example, calling \emph{createVertexBuffer(\ldots)} on a \emph{Renderer} instance yields a \emph{VertexBuffer} instance with meaningful access methods, such as \emph{mapRange} and \emph{setSubData} -- it is however only a thin \textbf{struct} wrapping a resource handle. The methods within it only pass the parameters along with the handle to the API provided by the \emph{Renderer}.

Additionally, the \textbf{Renderer} provides functionality to create, dispose and render the contents of a \textbf{RenderList} according to the current \textbf{RenderState}.

The \textbf{RenderList} is a collection of indices (ordinals) of EffectInstances and basic associated data required to render them, including:
	
\begin{itemize}
\item Model $\rightarrow$ world and world $\rightarrow$ model transformation 4Ã—3 matrices
\item An IndexBuffer along with the count of indices to use, offset, min and max therein
\item The number of instances of the object to be rendered via \emph{hardware instancing}
\item Mesh topology
\end{itemize}

An important factor to note is that the RenderList does not plainly contain EffectInstances, but rather their \emph{rendering ordinals}. The ordinals are 32-bit numbers assigned to each effect instance and managed internally by the renderer. Their order is determined by a heuristic attempting to minimize the number of state changes required when rendering the instances. This means that once the render list is constructed, the algorithm to minimize the required state changes basically boils down to \emph{sorting the ordinals}, which is a cheap operation. More ordering options will be made available in the future, so that the objects may be sorted by distance or using per-Effect routines built with knowledge of their performance characteristics.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.9\linewidth]{./Figures/terrainRendering.jpg}
    \caption[terrainRendering]{A terrain rendering example implemented using the low-level rendering API wrapper.}
  \label{fig:terrainRendering}
\end{figure}

As mentioned before, the \textbf{Renderer} also gives access to the \textbf{RenderState}, a set of common rendering settings, such as the mode of depth-testing, blending, back-face culling, etc.

The graphics base layer can therefore be used instead of the underlying graphics API, completely hiding its complexity and error-prone setup. It also makes it possible to change the backend without touching client code. The following is a code fragment of a Chunked LoD \cite{chunkedLoD} terrain rendering example (shown in figure \ref{fig:terrainRendering}) implemented using the low-level interface only:
\begin{lstlisting}[frame=single]
efInst = renderer.instantiateEffect(effect);
EffectHelper.allocateDefaultUniformStorage(efInst);

vb = renderer.createVertexBuffer(
	BufferUsage.StaticDraw,
	cast(void[])positions
);
va = VertexAttrib(
	0,
	vec3.sizeof,
	VertexAttrib.Type.Vec3
);

final vdata = efInst.getVaryingParamData("VertexProgram.input.position");
vdata.buffer = &vb;
vdata.attrib = &va;
\end{lstlisting}
The above sample instantiates an \emph{Effect} containing a vertex and a fragment shader, then creates a vertex buffer and finally binds a varying parameter in the vertex shader to a portion of that buffer. Such code would be considerably longer if the underlying OpenGL and Cg APIs were used instead.
	
\section{Kernels}
\label{sec:Kernels}

The basic building block of the high level rendering interface of Nucleus is called a \emph{kernel}. Conceptually, it is just a function. The implementation of a kernel may be specified using a code snippet (the current implementation allows the \emph{Cg} language for this purpose). The more interesting option is to define it using a graph. Specifically, a directed acyclic graph whose nodes are either kernels or special-purpose entities. The special-purpose nodes can be used to denote what constitutes the input and output parameters of a graph-based kernel, as well as to request external data for it.

While first-class support of graphs does not result in more expressive power than that of regular function-based composition, it provides a comfortable mental and programmatic framework for such composition. It enables the use of classical graph algorithms and makes development of authoring tools easier.

As a convention, the return values of kernels are called \emph{output parameters} and denoted by the \textbf{out} keyword in the kernel signature. Conversely, regular kernel parameters are called \emph{input parameters} and optionally denoted by the \textbf{in} keyword. This is modeled after the \emph{Cg} shading language.

Within a kernel graph, connections may exist between nodes, as well as between particular parameters of the nodes. The implications of this decision are discussed further in this chapter.

\subsection{Standard kernel types}

Similarly to how GPU-based shading requires the specification of shader programs for each of the processing domains, Nucleus demands that a kernel be provided for each of its domains. Where it differs drastically, is what the domains are. Each object to be rendered must have 3 kernels specified for it:

\begin{description}
\item[Structure] --- Defines the macro-structure and mostly contains what a vertex or geometry shader might. It is responsible for providing the primitives to be rendered, including data such as positions, normals, partial derivatives of position with respect to texture coordinates, etc.

\item[Reflectance] --- Enables the object to interact with lights by implementing a reflection model. Examples include the Lambertian model, Blinn-Phong, Cook-Torrance and Ashikhmin-Shirley, all of which have been implemented in Nucleus.

\item[Material] --- This is the level on which most artists will work, specifying the albedo of the rendered object, its bumpiness, specular tint, emissive lighting, roughness, etc. Kernels of this type will be created for types of objects in a scene, as well as for particular instances thereof.
\end{description}

Specifying these kernels for a renderable object boils down to designating a \emph{concrete} kernel with a compatible signature for each of the above \emph{abstract} kernels.

A scene will usually also contain lights, each of which must be associated with a kernel. In this case, an implementation of the abstract \emph{Light} kernel. At program runtime, \emph{Light} kernels are connected to the per-object \emph{Reflectance} kernels. This mechanism allows specification of custom attenuation, sampling and shadowing algorithms, which automatically are applicable to any \emph{Reflectance} kernel.

% TODO: link to info about the signatures of these kernels in some other chapter

Such a break-down of domains may superficially look similar to the shader types GPUs require. \emph{Structure} kernels resemble \emph{vertex} shaders, \emph{Material} kernels resemble \emph{pixel} shaders. The first major distinction is that the signatures of the above kernels are fixed in Nucleus, hence each Material may be combined with any Reflectance, any Structure and any Light. The second important difference is that these kernels are not tied to any particular part of the pipeline. Instead, a \emph{Renderer} is free to use these kernels in any rendering algorithm, as detailed in section \ref{sec:Renderers}.

Another core kernel type is used in \emph{post-processing} operations and described in section \ref{sec:PostProcessing}.

This particular breakdown of processing domains is comparable to the \emph{RenderMan} model:

\begin{center}
\begin{tabular}{ | c | c | c | }
\hline
RenderMan shader & Nucleus kernel \\
\hline
Surface & Material + Reflectance \\
Light & Light \\
Imager & PostProcess \\
Displacement & Structure \\
Volume & --- \\
\hline
\end{tabular}
\end{center}

Requiring that material and reflectance be defined by separate kernels gives Nucleus the ability to split the rendering equation into independent components and apply algorithms which utilize this distinction for performance gains in complex scene configurations -- for example, sections \ref{sec:ForwardRenderingExample} and \ref{sec:DeferredLightingExample} demonstrate how it allows Nucleus to use the same kernels for \emph{Forward} and \emph{Deferred} rendering, respectably. While it restricts the freedom of an artist to some extent, real-time rendering systems used in large, commercially successful computer games have often been limited to just a \emph{single} reflectance model \cite{CryEngine3Deferred, Killzone2Deferred}.

There is currently no support whatsoever for volume shaders in Nucleus. A proper solution would require ray marching in volumes and is beyond the scope of this thesis.

\section{Semantic type system}

% TODO: ref the related work section? maybe put semantic types into an index?
Nucleus uses \emph{semantic types} in order to describe the signatures of computational kernels. Each type is a set of \emph{trait} values and each \emph{trait} is an enumerated type in the classical meaning used in programming languages. Possible traits and their values include (TODO-ish):
\begin{description}
\item[basis] --- world, view, clip, tangent,
\item[colorSpace] --- RGB, sRGB, logLUV,
\item[length] --- unit, any,
\item[linearity] --- linear, logarithmic, exponential,
\item[use] --- position, normal, color, uv,
\item[type] --- float, float2, float3, float4, float4x4, ... (Cg type)
\end{description}

Trait values of the same name are considered distinct if they belong to different traits. For example, value \emph{A} of trait \emph{X} is distinct from value \emph{A} of trait \emph{Y}. In order to make the definition mathematically sound, a trait value may be considered a $(trait, value)$ tuple.

A type is then any combination of trait values, e.g. \emph{"type float3 + use position + basis world"} and \emph{"type float2 + use uv"}, where the "+" sign denotes a delimiter (actually, a semantic expression operator, as further described in section \ref{sec:SemanticExpressions}). An empty set of trait values is a valid type as well.

\subsection{Type coercion}
\label{sec:TypeCoercion}

A kernel graph specified by the user or constructed by an algorithm must be type-checked within Nucleus before it is used in any rendering operation. This section first explains the process for individual connections, then extends it to entire kernel graphs.

Given a connection from an output parameter of type $X$ to a input parameter of type $Y$, the connection is determined to be valid if $Y \subseteq X$. Otherwise such a connection is a type mismatch. For example, connecting an output parameter of type \emph{"type float3 + use position"} to an input parameter of type \emph{"type float3"} is valid, however connecting \emph{"type float4"} to \emph{"type float3"} or to \emph{"type float4 + use color"} is a mismatch.

In the case of a mismatch, Nucleus will try to perform \emph{type coercion} by inserting additional computational kernels between the source and the destination. The kernels which it considers come from a specifically designated set of \emph{semantic converters}. Each semantic converter is a kernel of a single input and a single output parameter. Additionally an integral \emph{cost} value is associated with each converter, which is an estimate of the computational complexity of the code within its definition.

\begin{figure}[h!]
  \centering
    \digraph[width=0.9\linewidth]{SemanticSearchGraph}{
		"1" [ fillcolor = "\#d0ffd0" label=<<font color="\#1040a0" >use</font> normal<br/><font color="\#1040a0" >basis</font> model> ];
		"1a" [ label=<<font color="\#1040a0" >use</font> normal<br/><font color="\#1040a0" >basis</font> model<br/><font color="\#1040a0" >unit</font> true> ];
		"2" [ label=<<font color="\#1040a0" >use</font> normal<br/><font color="\#1040a0" >basis</font> world> ];
		"2a" [ label=<<font color="\#1040a0" >use</font> normal<br/><font color="\#1040a0" >basis</font> view> ];
		"2b" [ label=<<font color="\#1040a0" >use</font> normal<br/><font color="\#1040a0" >basis</font> clip> ];
		"3" [ fillcolor = "\#d0ffd0" label=<<font color="\#1040a0" >use</font> normal<br/><font color="\#1040a0" >basis</font> world<br/><font color="\#1040a0" >unit</font> true> ];
		"1" -> "2" [ label="modelToWorld" ];
		"1" -> "1a" [ label="normalize" ];
		"2" -> "2a" [ label="worldToView" ];
		"2a" -> "2b" [ label="viewToClip" ];
		"2" -> "3" [ label="normalize" ];
	}
    \caption[Implicit semantic search tree]{An example slice of the implicit semantic conversion graph.}
  \label{fig:SemanticSearchGraph}
\end{figure}

It is possible to define a directed weighted graph $G=(V, E)$ such that its vertices, $V$ are all the possible semantic types. The graph contains an edge $e$ from $a$ to $b$ if and only if there exists a converter, which turns type $a$ into a type which may be connected to type $b$ without a mismatch. The weight associated with $e$ is the cost value of the corresponding converter.

Because $G$ contains all possible semantic types, it must contain $X$ and $Y$. A search for the shortest path is performed using Dijkstra's algorithm. A fragment of the implicit search tree is visualized in figure \ref{fig:SemanticSearchGraph}. If a path from $X$ to $Y$ in $G$ does not exist, an error is reported. If two or more shortest paths are found, an ambiguity error is reported. Otherwise, the path defines a valid \emph{type coercion}. New nodes are created within the kernel graph and inserted in place of the original parameter connection.
	
\subsection{Automatic connection determination}

The kernel graph may contain connections not only between particular parameters, but also between nodes. The type-checking process converts these coarse-grained dependencies into individual parameter connections using an extension of the type coercion algorithm.

\begin{figure}[h!]
  \centering
  \subfigure[Coarse dependency between nodes]{\label{fig:AutoSemanticGraph1}\digraph[width=0.7\linewidth]{AutoSemanticGraph1}{
    		"src" [
    			shape=rectangle style="rounded,filled"
    			label = <<table border="0" cellborder="0" cellspacing="8" >
    			<tr><td port="main" ><font point-size="14" >Source</font></td></tr>
    			<tr><td port="p1" ><font color="\#1040a0" >use</font> position + <font color="\#1040a0" >basis</font> model</td></tr>
    			<tr><td port="p2" ><font color="\#1040a0" >use</font> normal + <font color="\#1040a0" >basis</font> model</td></tr>
    			<tr><td port="p3" ><font color="\#1040a0" >use</font> color</td></tr>
    			</table>>
    		];
    		"dst" [
    			shape=rectangle style="rounded,filled"
    			label = <<table border="0" cellborder="0" cellspacing="8" >
    			<tr><td port="main" >Destination</td></tr>
    			<tr><td port="p1" ><font color="\#1040a0" >use</font> position + <font color="\#1040a0" >basis</font> world</td></tr>
    			<tr><td port="p2" ><font color="\#1040a0" >use</font> normal + <font color="\#1040a0" >basis</font> world + <font color="\#1040a0" >unit</font> true</td></tr>
    			<tr><td port="p3" ><font color="\#1040a0" >use</font> color</td></tr>
    			</table>>
    		];
    		src -> dst;
	}}
  \subfigure[The resulting fine-grained connections and conversions]{\label{fig:AutoSemanticGraph2}\digraph[width=0.9\linewidth]{AutoSemanticGraph2}{
    		"src" [
    			shape=rectangle style="rounded,filled"
    			label = <<table border="0" cellborder="0" cellspacing="8" >
    			<tr><td port="main" ><font point-size="14" >Source</font></td></tr>
    			<tr><td port="p1" ><font color="\#1040a0" >use</font> position + <font color="\#1040a0" >basis</font> model</td></tr>
    			<tr><td port="p2" ><font color="\#1040a0" >use</font> normal + <font color="\#1040a0" >basis</font> model</td></tr>
    			<tr><td port="p3" ><font color="\#1040a0" >use</font> color</td></tr>
    			</table>>
    		];
    		"dst" [
    			shape=rectangle style="rounded,filled"
    			label = <<table border="0" cellborder="0" cellspacing="8" >
    			<tr><td port="main" >Destination</td></tr>
    			<tr><td port="p1" ><font color="\#1040a0" >use</font> position + <font color="\#1040a0" >basis</font> world</td></tr>
    			<tr><td port="p2" ><font color="\#1040a0" >use</font> normal + <font color="\#1040a0" >basis</font> world + <font color="\#1040a0" >unit</font> true</td></tr>
    			<tr><td port="p3" ><font color="\#1040a0" >use</font> color</td></tr>
    			</table>>
    		];
    		src:p1 -> dst:p1 [ label="modelToWorld" ]
    		src:p2 -> dst:p2 [ label="modelToWorld, normalize" ];
    		src:p3 -> dst:p3;
	}}
  \caption[Automatic connection determination example]{A simple graph in which a dependency between nodes is specified in order to be resolved by the automatic connection determination algorithm into individual parameter connections.}
  \label{fig:AutoSemanticGraph}
\end{figure}

To automatically determine the parameter which should be connected to an input of a kernel graph node, all output parameters of all nodes connected to this node are considered. The possible type coercions are evaluated in parallel and the shortest one is selected. Similarly to the aforementioned algorithm, an ambiguous choice is an error. An example of this process is shown in figure \ref{fig:AutoSemanticGraph}.

\subsection{Semantic expressions}
\label{sec:SemanticExpressions}

Nucleus also extends the technique proposed in \cite{mcguire2006shadetrees} by allowing usage of simple expressions which operate on semantic traits in the specification of output parameter semantics.

When working with a prototype of Nucleus, which utilized a simpler version of the type system, it became apparent that it is useful not only to compute values using kernels, but also let the output semantics of kernel functions depend on the input semantics of these functions and the kernels they're connected to. For instance, One could have a kernel which samples a texture. Textures may be tagged with traits, specifying what sort of data they contain. If such a texture is connected to the sampling kernel, it is crucial to be able to express that the type of the sample should retain the traits of the texture. This is enabled by semantic expressions.

The sampling kernel could have the following signature:

\begin{lstlisting}[frame=single]
Tex2D = kernel (
    in texture <type sampler2D>,
    in uv <type float2 + use uv>,
    out sample <in.texture.actual + type float4>
);
\end{lstlisting}

% TODO: change other quotes to the `` ... '' style

In this case, the traits of the output parameter ``sample'' depend on the \emph{actual parameter} which is connected to the input ``texture'' parameter in a kernel graph. Hence, the result of plugging an input with the semantic \emph{<type sampler2D + use color>} will be a sample with the semantic \emph{<type float4 + use color>}.

There can be only one value for a particular trait within a semantic, and any duplicates within the expression replace the previous values. For example, \emph{<type float + type float2>} is equivalent to \emph{<type float2>}. This is useful if the semantic expression wants to add a particular trait, overriding whatever it was previously set to.

In addition to additive semantic expressions, subtractive ones are allowed. The semantic expression <type float - type> is an empty semantic. Parenthesizing sub-expressions is valid too, e.g. \emph{<type float + (in.foo.actual - type)>} is equivalent to \emph{<in.foo.actual + type.float>}, but due to non-associativity, distinct from \emph{<type float + in.foo.actual - type>}.

\section{Renderers}
\label{sec:Renderers}

Because of the particular split of shader domains which Nucleus utilizes, it is possible to perform scene rendering using various algorithms. Using an internal toolbox of kernel graph and type system operations, high-level client code may combine kernels into shader programs to fit in multiple rendering pipelines.

Since rendering algorithms will usually have similar requirements in terms of content they operate on, it's viable to extract a common interface. Therefore, Nucleus contains an abstract base \emph{Renderer} class, which allows the details of concrete implementations to be hidden. In end-user code the choice of a rendering algorithm only boils down to choosing an appropriate concrete sub-class.

Conceptually, a \emph{Renderer} is a straightforward entity. Besides being able to draw the scene for a particular viewpoint using a set of visible objects, it must support miscellaneous operations, like:

\begin{itemize}
\item Registration of materials, surfaces, renderables and lights
\item Invalidation of the above, as to support hot-swapping of configuration data
\item Custom implementation-dependent configuration
\end{itemize}

Within this thin framework, implementations may choose to perform arbitrary caching of shader and geometric data. The API can remain this small because many render state settings are already exposed by the low-level rendering API described in section \ref{sec:lowLevelInfo}. For example, in order to render into a specific framebuffer, it is sufficient to configure it in the low-level backend.

Ultimately, the work a \emph{Renderer} does is relatively straightforward -- it only needs to generate a complete kernel graph for each render stage (e.g. one in the \emph{forward} algorithm, three for \emph{light pre-pass}), then submit it to the separate \emph{code generation} pass, as described in section \ref{sec:codegen}. After doing this, its job is relatively simple due to the \emph{Effect}-based interface which the low-level backend provides.

Chapter \ref{Chapter6} describes \emph{Renderer} implementations for \emph{forward} and \emph{light pre-pass} algorithms, as well as a specialized shadow map renderer. The latter is a particularly interesting case, since it only uses the \emph{Structure} kernel of each renderable, skipping any other unnecessary calculations as to only compute per-pixel depth information.

Despite the \emph{Renderer} framework being the default for scene rendering operations, it's not the only way to utilize renderables and kernels associated with them. The same set of tools may be used with little effort for other rendering purposes. Both the \emph{post processing} pipeline described in the next section and material previews in the \emph{Nucled} editor are internally very similar to regular renderers.

Had there been need to access any intermediate results of rendering algorithms, the \emph{Renderer} interface may be skipped altogether. For the purposes of this dissertation, having a unified interface over renderers is a desirable property, however it may not always be so. There exist algorithms, which weave between steps of deferred rendering or utilize the created G-buffers for final image manipulation purposes; examples include volumetric decal rendering \cite{VolumeDecals} and horizon-based ambient occlusion \cite{HBAO}. Nevertheless, only the abstraction of a \emph{Renderer} would need to be surpassed for these techniques -- internally, they can still use the kernel-based shader generation mechanism and benefit from the other components of Nucleus.

\section{Post-processing}
\label{sec:PostProcessing}

In addition to regular scene rendering, Nucleus contains special support for image manipulation. Be that a video game or a film production, the results of 3D rendering are usually touched up by several filters put together by artists and technical directors. There's a multitude of motivations behind doing so:

\begin{itemize}
\item The output device usually has a limited display range with respect to brightness. For optimal results, the rendered values must be translated into this range via \textbf{tone mapping}.
\item \textbf{Psychophysical effects} may be simulated, such as glare, spatial acuity loss and scotopic vision.
\item Color grading, contrast enhancement and other stylistic effects may be applied in order to increase \textbf{immersion} and steer the \textbf{mood} of a particular scene.
\item Visual \textbf{gameplay cues} may be provided seamlessly to the player without resorting to artificial bars or number displays. For example, it's common practice to indicate low health of the player in a first person game by blurring the screen, as well as adding bloody detail to the edges of the screen.
\item \textbf{Attention focusing} via depth-of-field and selective coloring is often used in cutscenes and gameplay.
\end{itemize}

Similarly to regular rendering operations, post-processing in Nucleus is done within the framework of kernel graphs and operations thereon. A special \emph{PostProcessor} class is provided, which gives an easy interface for using kernel graphs in image filtering operations. It only requires that an input \emph{Texture} and an \emph{output} Framebuffer be specified along with a kernel matching the \textbf{PostProcess} signature:

\begin{lstlisting}[frame=single]
PostProcess = kernel(
    in  input  <type Image>,
    in  size   <type float2>,
    out output <type Image>
);
\end{lstlisting}

\begin{figure}[h!]
  \centering
  \subfigure[Post-processing off]{\label{fig:UBotPostOff}\includegraphics[width=0.45\linewidth]{./Figures/UBotPostOff.jpg}}
  \subfigure[Post-processing on]{\label{fig:UBotPostOn}\includegraphics[width=0.45\linewidth]{./Figures/UBotPostOn.jpg}}
  \caption[The impact of post-processing]{The impact of post-processing on a rendered scene. ``UBot'' model courtesy of Eric Andrew.}
\label{fig:lppFalloffBetterCase}
\end{figure}

Figure \ref{fig:SimplePostProc} shows an example of a post-processing pipeline. The crucial item to notice in this graph is the \textbf{Blit} node. It's not a concrete kernel with an implementation in code, but a special entity which only the \emph{PostProcessor} understands. Such nodes are used to break the graph to be rendered in multiple passes. Since certain classes of image convolution operators are \emph{linearly separable}, they may be applied to a two-dimensional image as two single-dimensional operations. This is just what happens in this graph. Had the \emph{Blit} node been removed, the pipeline would implement a regular two-dimensional blur. With the \emph{Blit} in place, an equivalent operation \emph{(for all practical purposes)} is done an order of magnitude more efficiently. Figure \ref{fig:SimplePostProcBreakdown} demonstrates the rendering steps which the graph from figure \ref{fig:SimplePostProc} is broken down into.

\begin{figure}[h!]
  \centering
    \digraph[width=0.9\linewidth]{SimplePostProc}{
		"Blit" [
			fillcolor = "\#d0ffd0"
		];
		"Input" -> "Horizontal Blur";
		"Horizontal Blur" -> "Blit";
		"Blit" -> "Vertical Blur";
		"Vertical Blur" -> "Combine";
		"Input" -> "Combine";
	}
    \caption[Simple post-processing]{A simple post-processing pipeline}
  \label{fig:SimplePostProc}
\end{figure}

In addition to causing multi-pass rendering, \emph{Blit} nodes may be used to rescale the images and change their internal format. A simple extension of the post-processing kernel shown in figure \ref{fig:SimplePostProc} could decide to output an image shrunk by 50\% on each side. With a sufficiently wide blur kernel, high-frequency details are lost anyway, so the signal may be represented by a smaller image within reasonable accuracy. Furthermore, combining the operator with itself a few times will approximate a blur with a very wide kernel in a much faster manner than a constant-resolution filter might.

\begin{figure}[h!]
  \centering
    \digraph[width=0.7\linewidth]{PostProcMRT}{
		subgraph "cluster_MRT" {
			label = "rendered simultaneously"
			"Input";
			"Calc1" [shape = none style = "" label = "..."];
			"Calc2" [shape = none style = "" label = "..."];
			"Blit1" [ label = "Blit" fillcolor = "\#d0ffd0" ];
			"Blit2" [ label = "Blit" fillcolor = "\#d0ffd0" ];
		}
		"Input" -> "Calc1";
		"Input" -> "Calc2";
		"Calc1" -> "Blit1";
		"Calc2" -> "Blit2";
		"Blit1" -> "Calc3";
		"Blit2" -> "Calc3";
		"Calc3" -> "Output";
	}
    \caption[MRT usage in post-processing]{A simple post-processing pipeline in which multiple operations may be processed in the same pass through the usage of the \emph{Multiple Render Targets} functionality of GPUs.}
  \label{fig:PostProcMRT}
\end{figure}

When breaking down the post-processing pipeline into multiple passes, Nucleus will attempt to merge compatible and legal operations into a single pass via \emph{Multiple Render Targets} (\textbf{MRT}). An example is given in figure \ref{fig:PostProcMRT}. In this case, multiple textures are attached to a single framebuffer and their values are written by outputting a matching count of colors from the generated fragment shader. The legality is determined by the topological ordering of filters, as well as by considering the limitations of \emph{MRT}. For example, current GPUs require that all render targets have the same dimensions.

% TODO: describe the algorithm for breaking down processing into passes

\section{Functional composition}
\label{sec:FunctionalComposition}

The post-processing framework required an introduction of one more element into the type system and graph manipulation toolbox. A limitation of the kernel pipeline described so far is that each kernel produces a single set of outputs for a single set of outputs. While this restriction is reasonable for implementing structure, material, light and reflectance shaders, it hinders elegant image processing.

Consider the post-processing pipeline presented in figure \ref{fig:TrickyPostProc}. The \textbf{Blur} kernel expects to receive an image which it may sample multiple times, with various offsets. Connecting the \emph{Input} directly to \emph{Blur} would have worked fine within the framework described so far. However, in this particular case, the user decided to filter the input image through a power function first. The function is not specialized to work on images, but on individual values. There's a clear type mismatch, but intuitively this case should work just fine.

\begin{figure}[h!]
  \centering
    \digraph[width=0.7\linewidth]{TrickyPostProc}{
		"Image" -> "pow2 (in float4, out float4)";
		"pow2 (in float4, out float4)" -> "Blur (in Image)";
	}
    \caption[A tricky post-processing pipeline]{A tricky post-processing pipeline}
  \label{fig:TrickyPostProc}
\end{figure}

The key to solving this issue lies in realizing what exactly is an image. In the particular case of shading languages, the answer is simple: it is a function of two-dimensional coordinates into colors, or more specifically, $float2 \Rightarrow float4$. If $input$ is the original image and $pow2$ is the power function, then what we really need to do is provide the $pow2 \circ input$ function to the \emph{Blur} kernel. This \emph{functional composition} is automatically performed by Nucleus.

Kernels within Nucleus are technically functions, so unsurprisingly, the \emph{Image} type seen in the aforementioned example is indeed a kernel; particularly, one with the following signature:
	
\begin{lstlisting}[frame=single]
Image = kernel(
    in uv <type float2 + use uv>,
    out sample <type float4 + use color>
);
\end{lstlisting}

The \emph{Blur} kernel in figure \ref{fig:TrickyPostProc} expects a \emph{kernel} as its input (it also expects two more parameters, which are irrelevant to the example), but the parameter connected to this input is not a kernel. Its type, however, is the type of the kernel's \emph{output} (\emph{float4}). What's more, the incoming subgraph contains a free ``float2 uv'' parameter, which is exactly what the \emph{Image} kernel needs. Hence despite the type mismatch, the entire subgraph connected to the final \emph{Blur} node can be converted to an \emph{Image}.

\begin{figure}[h!]
  \centering
    \digraph[width=0.8\linewidth]{TrickyPostProcResolve}{
		subgraph "cluster" {
			label = "function (in uv <type float2>, out sample <type float4>)"
			"uv" [ fillcolor = "\#909090" ];
			"Image";
			"pow2";
			"sample"  [ fillcolor = "\#909090" ];
		}
		"uv" -> "Image";
		"Image" -> "pow2";
		"pow2" -> "sample";
		"sample" -> "Blur";
	}
    \caption[The tricky post-processing pipeline resolved]{The tricky post-processing pipeline from figure \ref{fig:TrickyPostProc} resolved via functional composition}
  \label{fig:TrickyPostProcResolve}
\end{figure}

More specifically, when the kernel graph contains a connection from an output of type $V$ to an input of type $F = (P \rightarrow O)$, $V \neq F$, such that $c_O : V \rightarrow O$ is a valid conversion within the semantic type system, the coercion algorithm will attempt to perform functional composition of kernels. In order for it to succeed, the incoming subgraph from which the $V$-typed output came must contain a free parameter of type $I$ such that $c_I : P \rightarrow I$ is a valid conversion within the semantic type system.

Any kernel subgraph may be considered a function. Let's call the incoming subgraph from the aforementioned example $g$. If the criteria for kernel composition are satisfied, it's possible to construct a function $g\prime : P \rightarrow O = c_O \circ g \circ c_I$. This function is hence provided to the (higher-order) destination kernel and type coercion is complete.

Substituting $float2$ for $P$ and $I$, $float4$ for $O$ and $V$, as well as the identity function for $c_O$ and $c_I$, the particular example with image blurring from figure \ref{fig:TrickyPostProc} is achieved. The result is shown in figure \ref{fig:TrickyPostProcResolve}.

A requirement for functional composition is that the incoming subgraph contains free parameters. In order to prevent automatic flow determination from connecting anything to parameters meant to be used in composition, graph inputs may be marked to opt-out of automatic data flow. The kernel definition \emph{domain-specific language} provides a handy annotation to be used for this purpose.

%TODO
%More specifically, given a higher-order function $f : (A \rightarrow B) \rightarrow C$, and functions $g : D \rightarrow B$ and $h : A \rightarrow D$, the type system recognizes the invalid application $f(g(h))$, and turns it into $f(g \circ h)$.

Overall, the functional composition algorithm allows for a natural extension of graph-based processing, reducing the restriction of linear data flow in the \emph{DAG}, while still retaining functional purity, important for easy reasoning about and optimizations of the generated code. Conceptually, this feature is similar to automatic ``lifting'' of functions in some programming languages natively supporting arrays, such as \emph{APL} and \emph{J}.

Naturally this very mechanism is not restricted to just the post-processing component of Nucleus and may be used e.g. in order to implement procedural texturing. In this case, the \emph{Image} kernel may be used to abstract away regular texture access and procedural texture generation.
% TODO: ref the blurred checker example
	
\section{Code generation}
\label{sec:codegen}

Before anything can be rendered via kernels, they need to be translated to a form which a low-level rendering API (and hence the GPU) understands. This is done by generating shader code. The input to this final pass (henceforth called \emph{codegen}) is a complete kernel graph, with all conversions resolved and automatic flow deduced. Given this graph, the first step is determining which of its parts should run in which stages of the rasterization process.

A vertex shader processes each vertex in isolation and has a single mandatory output - the clip space position of the vertex. Such transformed vertex is then used by the primitive assembly stage of the GPU, which subsequently proceeds to scan-line conversion. Attributes output by the vertex shader are linearly interpolated across the primitive and made available to the fragment shader. The key observation here is that only the vertex position is special and must be made available in a particular format. All other outputs of the vertex shader are simply parameters which the fragment shader needs to generate the final color. It is then safe to assume that all computations of the kernel graph except the position may be done in the fragment stage. This assumption translates directly into the first step of the algorithm which determines the domain of each concrete kernel node:

\begin{enumerate}
\item Assume all nodes should run in the fragment domain.
\item Find a specially designated \textbf{Rasterize} node, mark it and all of the nodes in its incoming subgraph to run in the vertex domain.
\end{enumerate}

The \textbf{Rasterize} node is normally a part of a \emph{Structure} kernel graph specified for a renderable, but any node which accepts a position parameter may be used instead if a custom \emph{Renderer} deems it necessary. Due to the semantic type system, any data convertible to a clip space position may be supplied as the input.

Functions defined within the semantic type system do not need their storage types to be defined -- for example, an addition kernel should work on vectors, scalars and matrices, therefore it is a generic function. The type for its parameters is determined within the coercion mechanism described in section \ref{sec:TypeCoercion}. Therefore it may happen that the same kernel has to be generated in multiple instances, with types specialized for each fully coerced type. The code generation step resolves this gracefully by renaming functions in the target language and substituting types within the declarations.

Since graph-based kernels building up the final kernel graph for which codegen runs, the graph being compiled consists of only code-based kernels. Therefore it emits the code-based kernels as functions in the target language (\emph{Cg} currently) and creates glue code within entry points for each shader domain. The generated glue may look like:
\begin{lstlisting}[frame=single]
void FragmentProgram (
	in float2 n4__uv : TEXCOORD,
	out float4 bridge__0 : COLOR
) {
	Image n11__image;
	SamplerToImage(
		in__0tex,
		n11__image
	);
	// ...
	float4 n12__output;
	Blit_frag(
		n6__kernel,
		n4__uv,
		n12__output
	);
	bridge__0 = n12__output;
}
\end{lstlisting}
That is, \textbf{out} parameters are declared before each kernel function call, then passed to subsequent calls as arguments. At the end of the generated function, its outputs are gathered from the temporary variables created within the generated glue code, as can be seen in the last line of ``FragmentProgram'' in the above example. The extra variables and function calls only exist in the semantic phase of the high-level shading language compiler (\emph{cgc} in this case), but are inlined in the generated code, therefore do not cause any slowdown or bloat at runtime.

\subsection{Optimization due to linear functions}

The presented assignment of computational nodes to GPU domains is sufficient but not optimal. A typical scene rendering processes more fragments than vertices, so it is desirable to move calculations from the fragment to the vertex stage where possible. In order to show how this is possible, let's first take a closer look at the vertex and pixel processing pipeline from a mathematical point of view. Due to machine precision limits, the following consideration is strictly correct, but suffices for all practical purposes.

Let $f$ be the fragment shader, $v$ the vertex shader and $p_0 p_1 p_2$ the triangle being rasterized. The GPU computes $f(w_0 v(p_0) + w_1 v(p_1) + w_2 v(p_2))$ for each pixel of the triangle, where $w_0$, $w_1$ and $w_2$ are the barycentric coordinates of the center of the pixel with respect to $p_0 p_1 p_2$. Let now $g$ and $h$ be functions so that $h$ is linear and $g \circ h = f$. \\
Because $h$ is linear, \\
$g(h(w_0 v(p_0) + w_1 v(p_1) + w_2 v(p_2))) = g(w_0 h(v(p_0)) + w_1 h(v(p_1)) + w_2 h(v(p_2))$.

Subsequently, we can define a vertex shader function $v\prime = h \circ v$ and a fragment shader function $f\prime = g$. This result shows that it is legal to move a kernel from the fragment stage to the vertex stage as long as all of its inputs come from the vertex stage and the kernel implements a linear function. By utilizing this rule repeatedly in topological order, the process can be done in a single pass for the entire kernel graph. Nucleus currently allows a kernel to be tagged as linear, however in a future implementation this may be inferred automatically through semantic analysis of the code.

\begin{figure}[h!]
  \centering
  \subfigure{\label{fig:linFuncOpt1}\includegraphics[width=0.4\linewidth]{./Chapters/linearOptScr1.jpg}}
  \subfigure{\label{fig:linFuncOpt2}\includegraphics[width=0.4\linewidth]{./Chapters/linearOptScr2.jpg}}
  \caption{Scenes used to test the speedup from optimization due to linear functions}
  \label{fig:linFuncOpt}
\end{figure}

Figure \ref{fig:linFuncOpt} shows two scenes, which render respectively at 93 and 131 FPS without the optimization step. With it enabled, the performance reaches 96 FPS and 133 FPS respectively.

\subsection{Code generation for functional composition}

Functional composition does not require anything special from kernels, aside from them having the proper signatures. For instance, given the following ``Image'' kernel:
\begin{lstlisting}[frame=single]
Image = kernel(
    in uv <type float2 + use uv>,
    out sample <type float4 + use color>
);
\end{lstlisting}
One could construct a kernel which uses it like so:
\begin{lstlisting}[frame=single]
SampleImage = kernel (
	in image <type Image>,
	in uv <type float2 + use uv>,
	out sample <use color + in.image.actual + type float4>
) {
	sample = image.sample(uv);
};
\end{lstlisting}

Inline code within non-graph kernel bodies must be valid for the target language as to be emitted literally during the codegen process. Therefore the ``image.sample(uv)'' expression must be valid inside the generated function. This is an aspect which is not entirely satisfactory, because it ties some constructs of kernel implementations to the target language used during code generation. In this particular case, it requires that the target language supports method calls or expressions which look like them. If generalization to other shading languages is required, a more sophisticated codegen process may be used, however for the purposes of this thesis, \emph{Cg} is assumed.

In order for expressions using kernels to be valid in \emph{Cg}, \emph{interfaces} are generated. In the above case, the following interface would be defined:
\begin{lstlisting}[frame=single]
interface Image {
	float4 sample(float2 uv);
};
\end{lstlisting}

With this in place, the actual work done within functional composition may be hidden behind a concrete implementation of such an interface. For example, an implementation which returns blurred samples of another \emph{Image} might be emitted as:
\begin{lstlisting}[frame=single]
struct Image__impl6_g0 : Image {
	// ...
	float4 sample(float2 uv) {
		float4 n3__sample;
		BlurImage4__overload0_g1(
			uv,
			// ...
			n3__sample
		);
		return n3__sample;
	}
};
\end{lstlisting}

Again, interfaces only exist during semantic analysis of the code in \emph{Cg} and do not cause any overhead at runtime, which would be present in languages such as Java. Therefore the functional composition mechanism is a robust extension of linear data flow in a graph-based shading system.
