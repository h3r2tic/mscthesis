% Chapter 6

\chapter{ Examples }
\label{Chapter6}
\lhead{Chapter 6. \emph{ Examples }}

\section{Forward rendering}

Forward rendering is the classical algorithm utilized since the very beginning of rasterization and definitely the most commonly used one thus far. Since the introduction of GeForce 256 in late 1999, it has had special hardware and API support in consumer graphics devices. The special support is only being phased out recently due to programmable shading's ability of supporting the whole forward rendering algorithm efficiently. While modern rendering APIs such as \emph{Direct3D 10} and \emph{OpenGL 3.1's Core profile} no longer have dedicated support for transformation and lighting, the algorithm is still widely used and implemented via shaders. Despite other rendering algorithms being available, the performance characteristics of the forward approach make it favorable in many circumstances, hence it's imperative that Nucleus provides out of the box support for it.

The basic outline of the forward rendering algorithm is as follows:
	
\begin{algorithmic}
\FOR{each $object$ \textbf{in} $scene$}
	\STATE $lights$ $\Leftarrow$ determine-lights-influencing($object$)
	\STATE $radiance \Leftarrow 0$
	\FOR{each $light$ \textbf{in} $lights$}
		\STATE $radiance \Leftarrow radiance$ + \textbf{illuminate}($object$, $light$)
	\ENDFOR
	\STATE $output(radiance)$
\ENDFOR
\end{algorithmic}

The inner loop iterating all lights influencing an object can either can be approached in severals ways, for instance via multi-pass rendering, accumulating light influences into a \emph{Spherical Harmonics}-based approximation, approximating multiple lights into a single ``best-fit'' source, etc. The implementation in Nucleus uses a relatively simple approach of summing the contributions per-pixel on the GPU.

Iteration over a dynamically-sized array of lights in the fragment shader is an option, however it may be observed that for each light set influencing an object, all pixels will take the same branches. Hence it's desirable to unroll the loops and conditionals. This is done by generating a specialized shader for each set of influencing lights and statically evaluating all contributions.

\subsection{Kernel graph generation}

Each object has a \emph{Structure}, a \emph{Material} and a \emph{Reflectance} kernel associated with it. Each influencing light, a \emph{Light} kernel. The goal is simple then: given the set of kernels, create a single kernel graph describing the complete rendering operation. Such a graph may then be passed to the code generation subsystem in order to obtain GPU shaders.

As it turns out, this task is straightforward within the framework of Nucleus:
	
\begin{algorithmic}
\STATE $Structure \Rightarrow Material$
\STATE $radiance\_nodes \Leftarrow \emptyset$
\FOR{each $light$}
	\STATE $radiance\_nodes \Leftarrow radiance\_nodes \cup (light.kernel \Rightarrow Illumination)$
\ENDFOR
\STATE $Material \Rightarrow *$
\STATE $\textbf{foldl}(+, 0,radiance\_nodes) \Rightarrow *$
\STATE $* \Rightarrow \textbf{output}$
\end{algorithmic}

Where ``$\Rightarrow$'' denotes the operation of graph fusion and \textbf{foldl} is a functional reduction operator similar to the one used by the \emph{Haskell} programming language [TODO: ref], defined in terms of kernel graph composition.

\begin{figure}[h!]
  \centering
    \digraph[width=0.9\linewidth]{ForwardRenderingGraphSample}{
	"Reflectance1" [
		label = "Reflectance"
	];
	"Reflectance2" [
		label = "Reflectance"
	];
	"Structure" -> "Material";
	"Material" -> "Reflectance1";
	"Material" -> "Reflectance2";
	"Light1" -> "Reflectance1";
	"Light2" -> "Reflectance2";
	"Reflectance1" -> "+";
	"Reflectance2" -> "+";
	"+" -> "*";
	"Material" -> "*";
	"*" -> "output";
    }
    \caption[Forward rendering graph]{The composition of kernels for the \emph{Forward} rendering algorithm}
  \label{fig:ForwardRenderingGraphSample}
\end{figure}

Figure \ref{fig:ForwardRenderingGraphSample} shows an example of the kernel graph composition algorithm for the forward renderer.

After code generation, an \emph{Effect} is compiled (as described in section [TODO: ref the mid-level section]) and memoized using the input kernel set as the key. This allows more objects using the same combination of kernels to re-use the \emph{Effect} via instantiation, which is computationally inexpensive compared to the kernel graph processing algorithm. During run-time, new \emph{Effect}s are compiled only when a previously unencountered combination of lights, materials and reflectances appears.

% TODO: some timings for compiling kernel graphs, discussion about caching compiled shaders.

% TODO: mention of the block-based memory management and the resulting lack of fragmentation.

\clearpage
\section{Deferred lighting}

I have implemented a \emph{light pre-pass} renderer, which may be seamlessly used in place of the \emph{forward} algorithm. As described in section [TODO: ref intro], the algorithm is broken down into three stages. Similarly to the \emph{forward} renderer, the \emph{Effect}s used by this renderer and memoized using the appropriate kernels as keys for each stage.

\subsection{Geometry stage}

Most of the attributes required for the \emph{G-buffer} are computed by the \emph{Structure} kernel. Still, a \emph{Material} kernel may want to alter the normals in order to simulate a bumpy surface. These altered normals must be written into the \emph{G-buffer}. Hence, the first stage composes the two kernels and selectively writes their outputs to the \emph{G-buffer} as shown in Figure \ref{fig:DeferredLightingStage1}. The \emph{Material} kernel possibly writes more outputs than this stage uses, however, graph reduction is performed, so only the relevant parts are actually used at runtime. The \emph{Cg} compiler effectively removes any other extra computations.

\begin{figure}[h!]
  \centering
    \digraph[width=0.5\linewidth]{DeferredLightingStage1}{
	"Structure" -> "Material";
	"Material" -> "G-buffer";
    }
    \caption[Light Pre-Pass Stage 1]{The composition of kernels for the \emph{Geometry} stage of the  \emph{light pre-pass} rendering algorithm}
  \label{fig:DeferredLightingStage1}
\end{figure}

The \emph{G-buffer} has the following layout:

\begin{center}
\begin{tabular}{ | c | c | c | c | }
\hline
\multicolumn{4}{|c|}{ depth : f32 } \\
\hline
normal\_a:f16 & normal\_b:f16 & surface\_id:f16 & roughness:f16 \\
\hline
\end{tabular}
\end{center}

The normalized view-space normal is encoded in the \emph{G-buffer} using the \emph{Lambert Azimuthal Equal-Area projection} [TODO: ref Aras].

\subsection{Light stage}

Deferred renderers are notorious for being quite rigid with respect to illumination models achievable with them. This is because once the G-buffer is rendered, the light stage doesn't have access to per-object shaders, which might perform custom shading. Light volumes must then be rendered with the same shader for each pixel, regardless of which scene object a fragment in the G-buffer comes from.

A common approach is to use just one reflectance model ( usually the half-angle version of Phong's ) for all pixels and only control roughness. This may work just fine for games which don't require drastically varying surface styles.

Another solution is to render the majority of the scene using deferred rendering, but fall-back to forward rendering for surfaces requiring a different reflection model. Such an approach means that objects using the second-class reflectance models have to be used sparingly, and thus form a constraint imposed on artists.

The approach which Nucleus takes instead is not a novel one, but hasn't been widely adopted due to being costly on past generations of graphics hardware.

The G-buffer stores the \emph{surface ID} to be used at a given pixel and a fragment shader of the light stage branches on it, choosing the appropriate \emph{Reflectance} kernel implementation. The kernel index and parameters for it are stored in a texture, into which the \emph{G-buffer} contains a coordinate. The current approach allows up to 256 different surfaces, each of which might potentially use a different \emph{Reflectance} kernel. Obviously, this is not free either, however with the advent of real branching in recent GPU generations means that in practice the cost is not very high [TODO: numbers].

The code template used to compose multiple BRDFs together has the following structure:

\begin{lstlisting}[frame=single]
float surfaceId = tex2D(attribTex, uv);
float reflectanceId = tex2D(surfaceId, float2(surfaceId, 0));
 
if (reflectanceId < A) {
    result = BRDF1(
        tex2D(surfaceId, float2(surfaceId, ...)),
        tex2D(surfaceId, float2(surfaceId, ...)),
        tex2D(surfaceId, float2(surfaceId, ...))
    )
} else if (reflectanceId < B) {
    result = BRDF2(...);
} else if (reflectanceId < C) {
    ...
} else {
    ...
}
\end{lstlisting}

The code template includes dependent texture fetches and heavy branching. This is an obvious source of execution and memory fetch latency, however modern GPU architectures are able to hide some of it. [TODO: ref the latest Siggraph paper about GPU pipelines].
% TODO: move this to the future work section?
If this branching still turns out to be prohibitive, it's possible to reduce its cost using tile-based deferred rendering similar to [TODO: ref]. The framebuffer would be split into tiles, depending on whether they need the branch or not. Depending on the scene, many branches might use just a single BRDF, for which a specialized shader can be compiled.

As any of the other passes, the \emph{Light stage} generates a kernel graph (an example of which is shown in Figure \ref{fig:DeferredLightingStage2}), the only distinction is that it's compiled for the purpose of rendering the bounding volumes of lights.

\begin{figure}[h!]
  \centering
    \digraph[width=0.9\linewidth]{DeferredLightingStage2}{
	"G-buffer" -> "Reflectance1";
	"G-buffer" -> "Reflectance2";
	"G-buffer" -> "Reflectance3";
	"G-buffer" -> "Light";
	"Reflectance1" -> "switch";
	"Reflectance2" -> "switch";
	"Reflectance3" -> "switch";
	"switch" -> "Reflectance";
	"Light" -> "Reflectance";
	"Reflectance" -> "Illumination buffer";
    }
    \caption[Light Pre-Pass Stage 2]{The composition of kernels for the \emph{Light} stage of the  \emph{light pre-pass} rendering algorithm}
  \label{fig:DeferredLightingStage2}
\end{figure}

The current implementation uses a geometry shader to instantiate a cube for each light, taking a single point as the input. Only the quads facing away from the camera are drawn and reverse depth testing is performed in order to do approximate light culling. \\
Such setup will enable rendering of thousands of lights in a single batch, however this is not implemented yet. The downside of doing this for every light type would be large overdraw, since some lights may span a large portion of the screen and would benefit from more exact culling instead. Because of this, lights are actually drawn in separate batches, and approximate culling of their volumes is performed via the EXT\_depth\_bounds\_test OpenGL extension. Whereas the aforementioned reverse depth test allows culling of light volumes which are entirely in front of scene geometry, it doesn't do anything about the volumes being behind geometry. Depth bounds testing allows a range of depth values to be specified, so that no fragment is rendered if the value \emph{already in the depth buffer} is outside of this range. By conservatively specifying the minimum and maximum depth values of light volumes for this test, this effectively allows coarse culling. TODO: some picture.

The illumination is rendered to a framebuffer of the same resolution as the screen, with two floating-point RGB attachments --- one for the diffuse illumination component, another for the specular.

\subsection{Light bounding}

It's not physically correct to constrain light falloff to a volume, since real lights don't have a limited range. Still, at a certain distance, the influence becomes negligible and of little use as an artistic tool, while still having the same computational overhead as at meaningful intensities. Therefore, the size of the bounding volume is calculated such that it cuts off at a certain distance, which depends on the intensity of the source. It's impractical to set the fade-out distance as far as to avoid a perceptual difference, therefore the size is calculated aggressively (the current implementation cuts off at 1\% the normalized intensity) and light falloff is modified to smoothly disappear into zero.

In physics, the intensity of light is inversely proportional to the squared distance from the source:
	
\[
	I = \frac{L}{r^2}
\]

where $L$ is a constant representing the luminous intensity of the light.

Whereas this law may be used in its exact form (and many implementations do), it is important to notice that it is valid only for idealized \emph{point} light sources. The intensity at distance close to zero quickly approaches infinity, resulting in rendering artifacts. Real world lights have a non-zero area which makes the falloff look different, especially close to the source. For this reason, the model
\[
	I = \frac{L}{1 + r^2}
\]
is sometimes used in computer graphics. It has the property of approaching $L$ near the source, making it easier to achieve a consistent appearance of objects under varying conditions. The default light falloff in Nucleus is therefore based on this model. It is then modified to fade out smoothly to $0$ in a limited range:
\[
	I = \frac{L * (1 - s(r^2 / r_m^2))}{1 + r^2}
\]
Where $r_m$ is the radius bounding the influence of the light and $s : float \rightarrow float$ is the Hermite interpolation function: \emph{smoothstep(0, 1, \_)} (an intrinsic in\emph{Cg} and \emph{HLSL}):
\[
	s(t) =
	\begin{cases}
	0 & t <= 0 \\
	1 & t >= 1 \\
	t^2 * (3 - 2t) & otherwise \\
	\end{cases}
\]

TODO: a picture comparing the regular $1/r^2$ layout and the modified one

\subsection{Material stage}

Once the illumination buffer has been rendered, the final step is to combine it with scene object materials. In the light pre-pass algorithm, this is done by rendering the scene geometry again, in a similar fashion to regular forward rendering. The difference is that instead of computing the lighting, it's sampled from the previously rendered buffer. The diffuse part is multiplied by the material's albedo and the specular component -- by the specular tint of the material. The final radiance at the point is obtained by summing the two values.

\begin{figure}[h!]
  \centering
    \digraph[width=0.9\linewidth]{DeferredLightingStage3}{
    	"mul1" [
    		label = "*"
    	];
    	"mul2" [
    		label = "*"
    	];
	"Structure" -> "Material";
	"Diffuse illumination" -> "mul1";
	"Specular illumination" -> "mul2";
	"Material" -> "albedo";
	"albedo" -> "mul1";
	"Material" -> "specular tint";
	"specular tint" -> "mul2";
	"mul1" -> "+";
	"mul2" -> "+";
	"+" -> "output";
    }
    \caption[Light Pre-Pass Stage 3]{The composition of kernels for the \emph{Material} stage of the  \emph{light pre-pass} rendering algorithm}
  \label{fig:DeferredLightingStage3}
\end{figure}

\clearpage
\section{Shadows}

Shadows are an important part of 3-dimensional scene rendering. They provide visual cues relative object placement, help distinguish the shapes of objects and serve as a tool for mood shaping. Due to the direct integration of light sources which rasterization algorithms employ, shadows do not appear automatically, contrary to what might be expected from the rendering equation.

There are numerous ways of rendering shadows, however since in the context of this thesis they are only examples of using the rendering framework, I will not attempt to provide a survey of available methods. I will instead focus on \emph{shadow mapping}, a popular family of image-based algorithms offering a fine set of compromises for current hardware.

\subsection{A gentle introduction to standard shadow mapping}

Shadow mapping was first introduced by \citet{Williams78castingcurved} in 1978 and has since evolved into many similar algorithms. The following section contains a description of this standard algorithm, which is still in wide use due to its simplicity.

The core idea of shadow mapping is that of discretizing geometric information about a scene from the point of view of  light so that it may be quickly looked up when computing illumination during rendering. Assuming a point light, the task of determining whether a fragment is in shadow is equivalent to answering whether there is anything blocking a ray from this point to the position of the light (TODO: figure). Standard shadow mapping allows this question to be answered quickly by querying an image containing distances to the nearest objects from the point of view of the light.

Assuming that the light is a \emph{spot} with less than a 180$^\circ$ field of view, we can render an image containing distances to objects in the scene using regular rasterization and the \emph{depth-buffering} algorithm, which is very efficiently supported by graphics cards. The image contains distances to objects along rays starting at the light's position and traveling into the scene through positions of pixels in the virtual projection screen situated at the near clipping plane. The exact details to forming these rays are irrelevant here, the important detail is that they are effectively generated by the same perspective projection algorithm regular scene rendering uses. The resulting image is called a \textbf{depth map}.

During regular rendering, when a light's contribution to a fragment needs to be evaluated, this \emph{depth map} is used to approximately answer the visibility query. The fragment's world position is transformed via the same projection algorithm the depth map rendering used and as a result, a coordinate within the depth map is obtained. The pixel nearest to this coordinate contains the distance to the first object along the ray passing tho closest to the fragment out of all depth map samples. Comparing this stored value to the exact distance of the shaded fragment to the light (which is trivially calculable) answers the visibility test to within the precision of the depth map discretization. If the stored depth is smaller than the actual distance, the fragment is in shadow; otherwise, it is lit. In practice this comparison requires a slight bias due to the non-exact nature of the algorithm, but the core idea is just that.

TODO: more nice images

\subsection{Implementation of standard shadow mapping}

Most shadow mapping algorithms consist of two passes and the standard one is no exception. The first consists of rendering the shadow map, whereas the second pass ties into the evaluation of illumination when shading surfaces. The latter is a natural fit for implementing a custom \emph{light kernel}. The signature is:
	
\begin{lstlisting}[frame=single]
Light = kernel(
    in position <type float3 + use position + basis world>,
    out intensity <type float4 + use color>,
    out toLight <type float3 + basis world + use toLight>
);
\end{lstlisting}

The only output which will need to be calculated differently from regular, unshadowed rendering is \emph{intensity}. In order to compute it, the kernel will need information about the light instance in the first place. The \emph{position} parameter in the above kernel is only the position of the point being shaded, so all data for a light instance has to be defined within the implementation of the kernel.

In section \ref{sec:Kernels}, it was mentioned that kernels can receive external data for their calculations. This is accomplished through \textbf{Data} graph nodes. These nodes provide \emph{uniform} parameters into the pipeline and their values need to be explicitly handled by an appropriate CPU-side entity associated with the particular kernel type. In case of \emph{light} kernels, these are \emph{Light} classes and instances of their sub-types. Besides preparing and binding attributes to kernels, these CPU-side entities allow easy manipulation of lights from the logical side of the application.

As for shadow map generation, the solution was mentioned in section \ref{sec:Renderers} and boils down to creating a custom \textbf{Renderer}. The implementation is much simpler than that of the \emph{forward} and \emph{light pre-pass} algorithms, because it only uses \emph{structure} kernels associated with \emph{renderables}. Since \emph{structure} kernels must output final positions of the geometry, they need only be connected to a simple kernel which calculates distances from the light.

Finally, this specialized \textbf{DepthRenderer} is used in a custom \textbf{SpotLight\_SM} class on the CPU side, which in turn is a fa√ßade for a GPU kernel of the same name. Figure (TODO) demonstrates this relationship.

Upon rendering a scene, the set of visible lights is determined and each \emph{Light} sub-class instance is given a chance to prepare data for the GPU phase of rendering. This is where instances of the custom class will perform depth map rendering.

TODO: pictures with results

\subsection{Variance shadow mapping}

One major drawbacks of standard shadow mapping is that depth maps cannot be filtered in the same way as color textures. This typically leads to severe aliasing. Filtering methods based on sampling the depth map multiple times per a shaded fragment, computing the visibility query and averaging the result provide acceptable results, but at a heavy price (TODO: ref Advanced Soft Shadow Mapping Techniques - NVIDIA).

\citet{Donnelly06varianceshadow} present an interesting algorithm, which is simple to implement and solves the aliasing problem with minimal additional storage and computation. blah blah blah duh *SLAM* TODO

\clearpage
\section{Post-processing effects}

Several post-processing effects were implemented in order to test the pipeline.

TODO: \emph{really} short descriptions and screenshots for:
	
\begin{enumerate}
\item negative - done, trivial
\item desaturation - done, trivial
\item blur - done
\item glare - done
\item noise - done
\item distortion - 10 minutes
\end{enumerate}
