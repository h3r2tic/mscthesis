% Chapter 6

\chapter{ Examples }
\label{Chapter6}
\lhead{Chapter 6. \emph{ Examples }}

\section{Forward rendering}

Forward rendering is the classical algorithm utilized since the very beginning of rasterization and definitely the most commonly used one thus far. Since the introduction of GeForce 256 in late 1999, it has had special hardware and API support in consumer graphics devices. The special support is only being phased out recently due to programmable shading's ability of supporting the whole forward rendering algorithm efficiently. While modern rendering APIs such as \emph{Direct3D 10} and \emph{OpenGL 3.1's Core profile} no longer have dedicated support for transformation and lighting, the algorithm is still widely used and implemented via shaders. Despite other rendering algorithms being available, the performance characteristics of the forward approach make it favorable in many circumstances, hence it's imperative that Nucleus provides out of the box support for it.

The basic outline of the forward rendering algorithm is as follows:
	
\begin{algorithmic}
\FOR{each $object$ \textbf{in} $scene$}
	\STATE $lights$ $\Leftarrow$ determine-lights-influencing($object$)
	\STATE $radiance \Leftarrow 0$
	\FOR{each $light$ \textbf{in} $lights$}
		\STATE $radiance \Leftarrow radiance$ + \textbf{illuminate}($object$, $light$)
	\ENDFOR
	\STATE $output(radiance)$
\ENDFOR
\end{algorithmic}

The inner loop iterating all lights influencing an object can either can be approached in severals ways, for instance via multi-pass rendering, accumulating light influences into a \emph{Spherical Harmonics}-based approximation, approximating multiple lights into a single ``best-fit'' source, etc. The implementation in Nucleus uses a relatively simple approach of summing the contributions per-pixel on the GPU.

Iteration over a dynamically-sized array of lights in the fragment shader is an option, however it may be observed that for each light set influencing an object, all pixels will take the same branches. Hence it's desirable to unroll the loops and conditionals. This is done by generating a specialized shader for each set of influencing lights and statically evaluating all contributions.

\subsection{Kernel graph generation}

Each object has a \emph{Structure}, a \emph{Material} and a \emph{Reflectance} kernel associated with it. Each influencing light, a \emph{Light} kernel. The goal is simple then: given the set of kernels, create a single kernel graph describing the complete rendering operation. Such a graph may then be passed to the code generation subsystem in order to obtain GPU shaders.

As it turns out, this task is straightforward within the framework of Nucleus:
	
\begin{algorithmic}
\STATE $Structure \Rightarrow Material$
\STATE $radiance\_nodes \Leftarrow \emptyset$
\FOR{each $light$}
	\STATE $radiance\_nodes \Leftarrow radiance\_nodes \cup (light.kernel \Rightarrow Illumination)$
\ENDFOR
\STATE $Material \Rightarrow *$
\STATE $\textbf{foldl}(+, 0,radiance\_nodes) \Rightarrow *$
\STATE $* \Rightarrow \textbf{output}$
\end{algorithmic}

Where ``$\Rightarrow$'' denotes the operation of graph fusion and \textbf{foldl} is a functional reduction operator similar to the one used by the \emph{Haskell} programming language [TODO: ref], defined in terms of kernel graph composition.

\begin{figure}[h!]
  \centering
    \digraph[width=0.9\linewidth]{ForwardRenderingGraphSample}{
	"Reflectance1" [
		label = "Reflectance"
	];
	"Reflectance2" [
		label = "Reflectance"
	];
	"Structure" -> "Material";
	"Material" -> "Reflectance1";
	"Material" -> "Reflectance2";
	"Light1" -> "Reflectance1";
	"Light2" -> "Reflectance2";
	"Reflectance1" -> "+";
	"Reflectance2" -> "+";
	"+" -> "*";
	"Material" -> "*";
	"*" -> "output";
    }
    \caption[Forward rendering graph]{The composition of kernels for the \emph{Forward} rendering algorithm}
  \label{fig:ForwardRenderingGraphSample}
\end{figure}

Figure \ref{fig:ForwardRenderingGraphSample} shows an example of the kernel graph composition algorithm for the forward renderer.

After code generation, an \emph{Effect} is compiled (as described in section [TODO: ref the mid-level section]) and memoized using the input kernel set as the key. This allows more objects using the same combination of kernels to re-use the \emph{Effect} via instantiation, which is computationally inexpensive compared to the kernel graph processing algorithm. During run-time, new \emph{Effect}s are compiled only when a previously unencountered combination of lights, materials and reflectances appears.

% TODO: some timings for compiling kernel graphs, discussion about caching compiled shaders.

% TODO: mention of the block-based memory management and the resulting lack of fragmentation.

\clearpage
\section{Deferred lighting}

I have implemented a \emph{light pre-pass} renderer, which may be seamlessly used in place of the \emph{forward} algorithm. As described in section [TODO: ref intro], the algorithm is broken down into three stages. Similarly to the \emph{forward} renderer, the \emph{Effect}s used by this renderer and memoized using the appropriate kernels as keys for each stage.

\subsection{Geometry stage}

Most of the attributes required for the \emph{G-buffer} are computed by the \emph{Structure} kernel. Still, a \emph{Material} kernel may want to alter the normals in order to simulate a bumpy surface. These altered normals must be written into the \emph{G-buffer}. Hence, the first stage composes the two kernels and selectively writes their outputs to the \emph{G-buffer} as shown in Figure \ref{fig:DeferredLightingStage1}. The \emph{Material} kernel possibly writes more outputs than this stage uses, however, graph reduction is performed, so only the relevant parts are actually used at runtime. The \emph{Cg} compiler effectively removes any other extra computations.

\begin{figure}[h!]
  \centering
    \digraph[width=0.5\linewidth]{DeferredLightingStage1}{
	"Structure" -> "Material";
	"Material" -> "G-buffer";
    }
    \caption[Light Pre-Pass Stage 1]{The composition of kernels for the \emph{Geometry} stage of the  \emph{light pre-pass} rendering algorithm}
  \label{fig:DeferredLightingStage1}
\end{figure}

The \emph{G-buffer} has the following layout:

\begin{center}
\begin{tabular}{ | c | c | c | c | }
\hline
\multicolumn{4}{|c|}{ depth : f32 } \\
\hline
normal\_a:f16 & normal\_b:f16 & surface\_id:f16 & roughness:f16 \\
\hline
\end{tabular}
\end{center}

The normalized view-space normal is encoded in the \emph{G-buffer} using the \emph{Lambert Azimuthal Equal-Area projection} [TODO: ref Aras].

\subsection{Light stage}

Deferred renderers are notorious for being quite rigid with respect to illumination models achievable with them. This is because once the G-buffer is rendered, the light stage doesn't have access to per-object shaders, which might perform custom shading. Light volumes must then be rendered with the same shader for each pixel, regardless of which scene object a fragment in the G-buffer comes from.

A common approach is to use just one reflectance model ( usually the half-angle version of Phong's ) for all pixels and only control roughness. This may work just fine for games which don't require drastically varying surface styles.

Another solution is to render the majority of the scene using deferred rendering, but fall-back to forward rendering for surfaces requiring a different reflection model. Such an approach means that objects using the second-class reflectance models have to be used sparingly, and thus form a constraint imposed on artists.

The approach which Nucleus takes instead is not a novel one, but hasn't been widely adopted due to being costly on past generations of graphics hardware.

The G-buffer stores the \emph{surface ID} to be used at a given pixel and a fragment shader of the light stage branches on it, choosing the appropriate \emph{Reflectance} kernel implementation. The kernel index and parameters for it are stored in a texture, into which the \emph{G-buffer} contains a coordinate. The current approach allows up to 256 different surfaces, each of which might potentially use a different \emph{Reflectance} kernel. Obviously, this is not free either, however with the advent of real branching in recent GPU generations means that in practice the cost is not very high [TODO: numbers].

The code template used to compose multiple BRDFs together has the following structure:

\begin{lstlisting}[frame=single]
float surfaceId = tex2D(attribTex, uv);
float reflectanceId = tex2D(surfaceId, float2(surfaceId, 0));
 
if (reflectanceId < A) {
    result = BRDF1(
        tex2D(surfaceId, float2(surfaceId, ...)),
        tex2D(surfaceId, float2(surfaceId, ...)),
        tex2D(surfaceId, float2(surfaceId, ...))
    )
} else if (reflectanceId < B) {
    result = BRDF2(...);
} else if (reflectanceId < C) {
    ...
} else {
    ...
}
\end{lstlisting}

The code template includes dependent texture fetches and heavy branching. This is an obvious source of execution and memory fetch latency, however modern GPU architectures are able to hide some of it. [TODO: ref the latest Siggraph paper about GPU pipelines].
% TODO: move this to the future work section?
If this branching still turns out to be prohibitive, it's possible to reduce its cost using tile-based deferred rendering similar to [TODO: ref]. The framebuffer would be split into tiles, depending on whether they need the branch or not. Depending on the scene, many branches might use just a single BRDF, for which a specialized shader can be compiled.

As any of the other passes, the \emph{Light stage} generates a kernel graph (an example of which is shown in Figure \ref{fig:DeferredLightingStage2}), the only distinction is that it's compiled for the purpose of rendering the bounding volumes of lights.

\begin{figure}[h!]
  \centering
    \digraph[width=0.9\linewidth]{DeferredLightingStage2}{
	"G-buffer" -> "Reflectance1";
	"G-buffer" -> "Reflectance2";
	"G-buffer" -> "Reflectance3";
	"G-buffer" -> "Light";
	"Reflectance1" -> "switch";
	"Reflectance2" -> "switch";
	"Reflectance3" -> "switch";
	"switch" -> "Reflectance";
	"Light" -> "Reflectance";
	"Reflectance" -> "Illumination buffer";
    }
    \caption[Light Pre-Pass Stage 2]{The composition of kernels for the \emph{Light} stage of the  \emph{light pre-pass} rendering algorithm}
  \label{fig:DeferredLightingStage2}
\end{figure}

The current implementation uses a geometry shader to instantiate a cube for each light, taking a single point as the input. Only the quads facing away from the camera are drawn and reverse depth testing is performed in order to do approximate light culling. \\
Such setup will enable rendering of thousands of lights in a single batch, however this is not implemented yet. The downside of doing this for every light type would be large overdraw, since some lights may span a large portion of the screen and would benefit from more exact culling instead. Because of this, lights are actually drawn in separate batches, and approximate culling of their volumes is performed via the EXT\_depth\_bounds\_test OpenGL extension. Whereas the aforementioned reverse depth test allows culling of light volumes which are entirely in front of scene geometry, it doesn't do anything about the volumes being behind geometry. Depth bounds testing allows a range of depth values to be specified, so that no fragment is rendered if the value \emph{already in the depth buffer} is outside of this range. By conservatively specifying the minimum and maximum depth values of light volumes for this test, this effectively allows coarse culling. TODO: some picture.

The illumination is rendered to a framebuffer of the same resolution as the screen, with two floating-point RGB attachments --- one for the diffuse illumination component, another for the specular.

\subsection{Material stage}

Once the illumination buffer has been rendered, the final step is to combine it with scene object materials. In the light pre-pass algorithm, this is done by rendering the scene geometry again, in a similar fashion to regular forward rendering. The difference is that instead of computing the lighting, it's sampled from the previously rendered buffer. The diffuse part is multiplied by the material's albedo and the specular component -- by the specular tint of the material. The final radiance at the point is obtained by summing the two values.

\begin{figure}[h!]
  \centering
    \digraph[width=0.9\linewidth]{DeferredLightingStage3}{
    	"mul1" [
    		label = "*"
    	];
    	"mul2" [
    		label = "*"
    	];
	"Structure" -> "Material";
	"Diffuse illumination" -> "mul1";
	"Specular illumination" -> "mul2";
	"Material" -> "albedo";
	"albedo" -> "mul1";
	"Material" -> "specular tint";
	"specular tint" -> "mul2";
	"mul1" -> "+";
	"mul2" -> "+";
	"+" -> "output";
    }
    \caption[Light Pre-Pass Stage 3]{The composition of kernels for the \emph{Material} stage of the  \emph{light pre-pass} rendering algorithm}
  \label{fig:DeferredLightingStage3}
\end{figure}

\clearpage
\section{Shadows}

Intro to shadow mapping.

Regular shadow mapping.

Variance shadow mapping.

\clearpage
\section{Post-processing effects}
